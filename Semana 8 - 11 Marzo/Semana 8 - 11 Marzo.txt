Que es web mining ,web, categorias de mineria web 

# La legalidad de la descargar de datos.
#Sitios web que requiere que inicies sesión, ya que esto podría significar que ha aceptado unas Condiciones de Servicio que pueden prohibir las actividades de web scraping.
#Asegúrate de consultar las condiciones generales del sitio web para no incumplir el contrato.
#Aunque sean datos públicos, asegúrarse de que no estén protegidos por derechos de autor. Esto puede incluir artículos, vídeos y diseños.
#considerar la ética implicada. Aunque una actividad no sea ilegal, puede perjudicarle a usted o a otros o dañar su reputación.

# instalacion de las librerias 

library(RSelenium)
library(wdman)
library(netstat)
library(rvest)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)


selenium()
#inicializar 
selenium_object<-selenium(retcommand = T,check = F)

# configuracion con chrome
binman::list_versions("chromedriver")
remote_driver <-rsDriver(browser="chrome",
                         chromever="122.0.6261.94",
                         verbose=F,
                         port=free_port())

remote_driver$server$stop()

#Configuracion con Firefox

homepage<-"https://autos.mercadolibre.co.cr/autos-camionetas/"

remote_driver <-rsDriver(browser="firefox",
                         chromever=NULL,
                         verbose=F,
                         port=free_port())

remDr<- remote_driver$client
#remDr$open()
remDr$navigate(homepage)
Sys.sleep(2)
# maximizar la pagina 
remDr$maxWindowSize()


#inicializar un DF para almacenar la informacion 
cars <- data.frame(name = character(), price = character(),atributes = character(),location = character(),year= character(),kilometers= character(), stringsAsFactors = FALSE)

# Crear la funcion para la descarga de datos para llamarla cada vez que cargue una nueva pagina 

filldata<-function(homepage){
  link =unlist(homepage) 
  page = read_html(link)
  name=page %>% html_nodes(".poly-box") %>% html_text() %>% trimws()
  price=page %>% html_nodes(".poly-component__price") %>% html_text() %>% trimws()
  atributes=page %>% html_nodes(".poly-component__attributes-list") %>% html_text() %>% trimws()
  location=page %>% html_nodes(".poly-component__location") %>% html_text() %>% trimws()
  year=page %>% html_nodes(".poly-component__attributes-list") %>% html_text() %>% trimws()
  year=substr(year, 1, 4)
  kilometers=page %>% html_nodes(".poly-component__attributes-list") %>% html_text() %>% trimws() %>% gsub("Km", "", .) 
  kilometers=substr(kilometers, 5, nchar(kilometers))%>% trimws()
  new_car <- data.frame(name, price,atributes,location,year,kilometers, stringsAsFactors = FALSE)
  cars<- rbind(cars, new_car)
  return(cars)
}

#https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python

# cargamos los datos de primera pagina
cars<-filldata(homepage)

View( cars)

for (i in 1:2) {
  remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
  Sys.sleep(2)
  nextpage <- remDr$findElement(using = 'link text', 'Siguiente')
  # Comprobar si el elemento 'Siguiente' está presente
  if (!is.null(nextpage)) {
    newhomepage <- nextpage$getElementAttribute('href')
    Sys.sleep(2)
    nextpage$clickElement()
    cars <-filldata(newhomepage)
    Sys.sleep(5)
  } else {
    cat("No se encontró el enlace 'Siguiente'. Terminando el bucle.\n")
    break  # Rompe el bucle si no se encuentra el enlace 'Siguiente'
  }
}

View(cars)
# apliquemos alguna limpiezas

cars <- cars %>% mutate(currency = ifelse(grepl("\\$", price), "dolares", "colones"))
cars$price <-gsub("[₡$,US]", "", cars$price)
cars$price<-as.double(cars$price)
View(cars)

cars <- separate(cars, location, into = c("canton", "provincia"), sep = "-", remove = FALSE)

cars <- cars %>% 
  mutate(name = gsub("[[:punct:] ]", " ", name))

# primeras dos palabras pero me gusta mas la siguiente
cars <- cars %>% 
  mutate(marca = str_extract(name, "^\\S+\\s+\\S+"))

cars <- cars %>% 
  mutate(marca = word(name, 1))

View(cars)

# viengo algunas graficas

generate_plot <- function(data, variable,flip = FALSE) {
  plot_data <- data.frame(table(data[[variable]]))
  gg<-ggplot(plot_data, aes(x = reorder(Var1, -Freq), y = Freq, fill = Var1)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = Freq), vjust = -0.5, color = "black", size = 3) + 
    labs(title = paste("Cantidad de elementos por", variable),
         x = variable,
         y = "Cantidad") +
    theme_minimal()
    if (flip) {
    gg <- gg + coord_flip()
    }
  
  print(gg)
}


generate_plot(cars, "canton")
generate_plot(cars, "marca",TRUE)

cars %>% 
  group_by(name) %>% 
  summarise(total = n()) %>% 
  arrange(desc(total))

# general los histogramas

cars$price_millions <- cars$price / 1000000
hist(cars$price_millions, breaks = 30, xlab = "Precio", ylab = "Frecuencia", main = "Histograma de Precios", xaxt = "n")
axis(side = 1, at = pretty(cars$price_millions), labels = pretty(cars$price_millions))

# generar los boxplots

iqr <- IQR(cars$price_millions)
upper_limit <- quantile(cars$price_millions)[4] + 1.5 * iqr
ggplot(cars, aes(x = "", y = price_millions)) +
  geom_boxplot(fill = "green", color = "black", outlier.color = "red") +
  labs(title = "Boxplot de Precios con Valores Atípicos Resaltados",
       y = "Precio") +
  theme_minimal() +
  theme(axis.text.x = element_blank())


# Navegar en los elementos encontrados
Sys.sleep(5)
remDr$goBack()
Sys.sleep(5)
remDr$goForward()

# usar los buscadores de las paginas 
search_box<-remDr$findElement(using = 'id','cb1-edit')
search_box$sendKeysToElement(list('Mitsubichi Montero',key='enter'))

remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")

Sys.sleep(5)

# hacer click en los filtros de la pagina / si fuera un combobox es la clase input
#pero en esta pagina no hay , lo que hay son link entonce se hace con las a

usefilter <- remDr$findElement(using = 'xpath', '//a[@aria-label="Repuestos Autos y Camionetas"]')
#usefilter$getElementAttribute()  este sirve con otros componentes no con le  href
usefilter$clickElement()
remDr$goBack()

remote_driver$server$stop()
View(cars)


-------------------------------------------------------------------------

# limpiamos el workspace
rm(list = ls())
# Limpiamos la consola
cat("\014")
# fijamos a UTF-8
options(encoding = "utf-8")

#########################
# 1. Librerias necesarias
#########################

# Vamos a cargar las librerias necesarias
install.packages(c("pdftools","stopwords","tidytext",
                   "stringi","stringr","scales",
                   "tidyr","widyr","ggraph","igraph",
                   "quanteda","topicmodels","cvTools"))

library(pdftools)
library(dplyr)
library(stopwords)
library(tidytext)
library(stringi)
library(stringr)
library(ggplot2)
library(scales)
library(tidyr)
library(widyr)
library(ggraph)
library(igraph)
library(quanteda)
library(topicmodels)
library(cvTools)

########################
# 2. Carga de datos
########################

# Nos ubicamos en nuestro directorio de trabajo
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#getwd()

# Lectura de archivos de texto
texto01 <- pdftools::pdf_text("files/DONQUIJOTE_PARTE1.pdf")
texto02 <- pdftools::pdf_text("files/DONQUIJOTE_PARTE2.pdf")

# se refenciamos al titulo 
texto01[1]

# Tamaño de cada libro
length(texto01)
length(texto02)

texto <- c(texto01,texto02)

length(texto)

# Vamos a hacer un poco de limpieza de texto
# ------------------------------------------
texto <- gsub("\\r", " ", texto)
texto <- gsub("\\n", "", texto)
texto <- gsub("\\d\\K\\.(?=\\d)", "", texto, perl = TRUE)#  Los puntos de separador de mil, lo sustituimos por un espacio

# Juntamos todas las páginas del libros
texto<-paste(texto, collapse = '')
# ya no quedan 900 y resto si no solo 1
length(texto)

texto[1]

# Vamos a estructurar el texto
texto <- gsub("http://www.educa.jcyl.es","",texto)

# se va a considerar el punto como estructura de analisis
# osea se va a generar elementos por medio de frases que terminan en punto
vector = c()
for(i in 1:length(texto)){
  temp<-(strsplit(texto[[i]], "\\.")[[1]])
  print(temp)
  vector <- c(vector, temp)
}

# Lo convertimos a un dataframe
frases_texto<-as.data.frame(vector)
View(frases_texto)
# le cambiamos el titulo 
colnames(frases_texto)[1]<-"frase"

#####################################
# 3. Limpieza de texto y tokenizaci?n
#####################################

# Quitamos los espacios de encabezado y pie de página
frases_texto$frase<-trimws(frases_texto$frase, "l") # para la izquierda trimws(frase,'r')
# Convertimos a caracter
frases_texto$frase <-as.character(frases_texto$frase)

# Vamos a hacer un poco de limpieza de texto, retirando texto que no aporta
# como el titulo , pie de pagina , ect 
# ------------------------------------------
frases_texto$frase<-gsub("El Ingenioso Hidalgo Don Quijote de la Mancha","",frases_texto$frase)
frases_texto$frase<-gsub("PRIMERA PARTE","",frases_texto$frase)
frases_texto$frase<-gsub("Miguel de Cervantes Saavedra","",frases_texto$frase)
frases_texto$frase<-gsub("Portal Educativo EducaCYL","",frases_texto$frase)


#####################################
# 4. Analisis exploratorio de texto
#####################################
# Nos creamos un lexicon de stopwords en espa?ol 
# el stopword es una lexicologia
library(tm)

lexiconSW<-stopwords("es")
# si queremos agregar palabraas al SW manualmente 
lexiconSW <- append(lexiconSW,c("capítulo","d"," "))


lexiconSW<-as.data.frame(lexiconSW) # convertimos a dataframe
names(lexiconSW)<-"word"
lexiconSW$word<-as.character(lexiconSW$word)
View(lexiconSW)

#para revisar si la palabra ingresada manualmentre si quedo en el SW
lexiconSW %>% filter(lexiconSW$word=="tuvieras")


df <- tibble::rowid_to_column(frases_texto, "ID") #  Generamos un ID para cada frase
View(df)

# 1.1. Algunos análisis básicos
# -----------------------------
View(review_words)

review_words <- df %>%
  distinct(frase, .keep_all = TRUE) %>% # eliminar filas duplicadas basadas en frase
  unnest_tokens(word, frase, drop = FALSE) %>%# trabajar por tocket osea separar por la granularidad de la palabras
  distinct(ID, word, .keep_all = TRUE) %>% # se genera el desagregado por palabras
  anti_join(lexiconSW) %>% # devuelve todas las filas de x donde no hay valores coincidentes en y, manteniendo solo columnas de x.
  filter(str_detect(word, "[^\\d]")) %>% # selececciona words con algun comentario
  group_by(word) %>%
  dplyr::mutate(word_total = n()) %>%
  ungroup() # agrega nuevas variables y conserva las existentes

# Contamos las palabras resultantes
# ---------------------------------
word_counts <- review_words %>%
  dplyr::count(word, sort = TRUE)

word_counts %>%
  head(40) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "blue") +
  scale_y_continuous(labels = comma_format()) +
  coord_flip() +
  geom_text(
    label=n,
    nudge_x=0.45, nudge_y=0.1,
    check_overlap=T
  )+
  labs(title = paste0("Palabras mas utilizadas"),
       subtitle = "Stopwords retiradas",
       x = "Palabra",
       y = "Numero de veces usada")


word_counts %>%
  head(40) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "blue") +
  scale_y_continuous(labels = comma_format()) +
  coord_flip() +
  geom_text(
    aes(label=as.character(n),fontface="bold",hjust=-0.25,size = 2), # Modificación en esta línea
    nudge_x=0.45, nudge_y=0.1,
    check_overlap=T
  )+
  labs(title = paste0("Palabras mas utilizadas"),
       subtitle = "Stopwords retiradas",
       x = "Palabra",
       y = "Numero de veces usada")

# Generamos nuestro WordCloud
# ---------------------------
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)

df_grouped_V <- review_words %>% group_by(word) %>% count(word) %>%  
  group_by(word) %>% mutate(frecuencia = n/dim(review_words)[1])
# el mutate lo que hace es establecer el peso de la palabra en la nube

# Generamos el wordcloud
wordcloud(words = df_grouped_V$word, freq = df_grouped_V$frecuencia,
          max.words = 400, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"))

# 1.2. Bigramas Son analisis en pares de palabras
# -------------
# A veces nos interesa entender la relaci?n entre palabras en una opini?n. 
View(review_bigrams)

review_bigrams <- df %>%
  unnest_tokens(bigram, frase, token = "ngrams", 
                n = 2) # separamos token 2 - grams

bigrams_separated <- review_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") # separamos word por bigrama

View(bigrams_separated)


bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% lexiconSW$word) %>%
  filter(!word2 %in% lexiconSW$word) # eliminamos  stop words por bigrama
View(bigrams_filtered)

bigram_counts <- bigrams_filtered %>% 
  dplyr::count(word1, word2, sort = TRUE) # contamos la cantidad de words por bigrama
View(bigram_counts)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ") # count bigrams cleaning
View(bigrams_united)

bigrams_united %>%
  dplyr::count(bigram, sort = TRUE)


# Podemos visualizarlo tambien
review_subject <- df %>% 
  unnest_tokens(word, frase) %>% 
  anti_join(lexiconSW)

my_stopwords <- data_frame(word = c(as.character(1:10)))

review_subject <- review_subject %>% 
  anti_join(my_stopwords)

title_word_pairs <- review_subject %>% 
  pairwise_count(word, ID, sort = TRUE, upper = FALSE)

# Nos generamos el listado de bigramas con las tuplas de mas de 100 frecuencias
listadoBigramas<-title_word_pairs[which(title_word_pairs$n>200),]


set.seed(1234)

title_word_pairs %>%
  filter(n >= 100) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  ggtitle('Bigramas')